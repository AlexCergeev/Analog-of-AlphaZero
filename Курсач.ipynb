{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Курсач",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlexCergeev/Analog-of-AlphaZero/blob/main/%D0%9A%D1%83%D1%80%D1%81%D0%B0%D1%87.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYsbk_8vYy9k"
      },
      "source": [
        "**Start on GPU (Runtime shape High-RAM)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GQ2XbdEjTAd"
      },
      "source": [
        "# Import required libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iya2SZ92ilT8"
      },
      "source": [
        "You need to upload 3 files: \n",
        "\n",
        "1.   data.pgn\n",
        "2.   stockfish.csv\n",
        "3.   stockfish_13_linux_x64_bmi2.zim\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fn0sCWoHYsbB"
      },
      "source": [
        "!pip install python-chess==0.31.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xg1eanm3Sb_H"
      },
      "source": [
        "!unzip /content/stockfish_13_linux_x64_bmi2.zip\n",
        "!chmod +x /content/stockfish_13_linux_x64_bmi2/stockfish_13_linux_x64_bmi2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xt7U3FCjuAZ"
      },
      "source": [
        "import chess\n",
        "import chess.engine\n",
        "import chess.pgn\n",
        "import random\n",
        "from PIL import Image, ImageFilter\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import signal\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPooLxABj1_S"
      },
      "source": [
        "import tensorflow.keras.models as models\n",
        "import tensorflow.keras.layers as layers\n",
        "import tensorflow.keras.utils as utils\n",
        "import tensorflow.keras.optimizers as optimizers\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Conv2D, \\\n",
        "                                    Dense, \\\n",
        "                                    MaxPool2D,\\\n",
        "                                    Dropout, \\\n",
        "                                    Flatten, \\\n",
        "                                    BatchNormalization,\\\n",
        "                                    Input\n",
        "#from tensorflow.keras.datasets import mnist\n",
        "import tensorflow.keras.callbacks as callbacks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vJysv_aA8Me"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "import time\n",
        "import sys"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKMU04S7jGVS"
      },
      "source": [
        "# GPU Optimization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrIAQhPQjrcj"
      },
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  try:\n",
        "    # Currently, memory growth needs to be the same across GPUs\n",
        "    for gpu in gpus:\n",
        "      tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "  except RuntimeError as e:\n",
        "    # Memory growth must be set before GPUs have been initialized\n",
        "    print(e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cK4cp850Cpdg"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QauvWk2MkddY"
      },
      "source": [
        "# this function will create our f(x) (score)\n",
        "def stockfish(board):\n",
        "  with chess.engine.SimpleEngine.popen_uci('/content/stockfish_13_linux_x64_bmi2/stockfish_13_linux_x64_bmi2') as sf:\n",
        "    for i in range(19, 0, -1):    \n",
        "      result = sf.analyse(board, chess.engine.Limit(depth=i))\n",
        "      score = result['score'].white().score()\n",
        "      if score is not None:\n",
        "        break\n",
        "    return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULOEWyyfYqtq"
      },
      "source": [
        "board = chess.Board()\n",
        "board"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Fgr5NTWqgsA"
      },
      "source": [
        "**The AI will learn how to give a accurate prediction of *f(x)* when we present a *x* never seen before.**\n",
        "\n",
        "*board -> score*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMgBRL1Kdc0B"
      },
      "source": [
        "# Creating the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4Sg-yu0sS4t"
      },
      "source": [
        "Now we need to convert the board representation to something meaningful.\n",
        "\n",
        "A 3d matrix of sizes **14 x 8 x 8**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rdo64dA7dhBE"
      },
      "source": [
        "squares_index = {\n",
        "  'a': 0,\n",
        "  'b': 1,\n",
        "  'c': 2,\n",
        "  'd': 3,\n",
        "  'e': 4,\n",
        "  'f': 5,\n",
        "  'g': 6,\n",
        "  'h': 7\n",
        "}\n",
        "\n",
        "\n",
        "# example: h3 -> 17\n",
        "def square_to_index(square):\n",
        "  letter = chess.square_name(square)\n",
        "  return 8 - int(letter[1]), squares_index[letter[0]]\n",
        "\n",
        "\n",
        "def split_dims(board):\n",
        "  # this is the 3d matrix\n",
        "  board3d = np.zeros((14, 8, 8), dtype=np.int8)\n",
        "\n",
        "  # here we add the pieces's view on the matrix\n",
        "  for piece in chess.PIECE_TYPES:\n",
        "    for square in board.pieces(piece, chess.WHITE):\n",
        "      idx = np.unravel_index(square, (8, 8))\n",
        "      board3d[piece - 1][7 - idx[0]][idx[1]] = 1\n",
        "    for square in board.pieces(piece, chess.BLACK):\n",
        "      idx = np.unravel_index(square, (8, 8))\n",
        "      board3d[piece + 5][7 - idx[0]][idx[1]] = 1\n",
        "\n",
        "  # add attacks and valid moves too\n",
        "  # so the network knows what is being attacked\n",
        "  aux = board.turn\n",
        "  board.turn = chess.WHITE\n",
        "  for move in board.legal_moves:\n",
        "      i, j = square_to_index(move.to_square)\n",
        "      board3d[12][i][j] = 1\n",
        "  board.turn = chess.BLACK\n",
        "  for move in board.legal_moves:\n",
        "      i, j = square_to_index(move.to_square)\n",
        "      board3d[13][i][j] = 1\n",
        "  board.turn = aux\n",
        "\n",
        "  return board3d\n",
        "\n",
        "#This function was made by owner of chennel \"Digital Secrets\"\n",
        "#(original code: https://colab.research.google.com/drive/1GSeBQdyZH_nHvl52XW0uhXV3Cslho24O)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwpzPcsXfuVw"
      },
      "source": [
        "Now, all we have to do is call **random_board()** to create random boards, **stockfish()** to get a score for how good each board is for white.\n",
        "\n",
        "Then we convert each board to a 3d matrix using **split_dims()**, now creating the dataset is easy!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukmA7z-dlB6m"
      },
      "source": [
        "# TensorFlow!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S1RCn3xn4Bu"
      },
      "source": [
        "Skip connections (residual network) will likely improve the model for deeper connections. If you want to test the residual model, check the code below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ck79-w2ZxwVB"
      },
      "source": [
        "# It's training time!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkOXxmoVyHdc",
        "cellView": "both"
      },
      "source": [
        "# def get_dataset():\n",
        "# \tcontainer = np.load('/content/drive/My Drive/Chess/dataset.npz')\n",
        "# \tb, v = container['b'],container['v']\n",
        "# \tv = np.asarray(v / abs(v).max() / 2 + 0.5, dtype=np.float32) # normalization (0 - 1)\n",
        "# \treturn b, v\n",
        "\n",
        "# x_train, y_train = get_dataset()\n",
        "# print(type(x_train))\n",
        "# print(y_train.shape)\n",
        "# #TODO (y_train+50)/100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MckOBGpWfwTi"
      },
      "source": [
        "def get_dataset():\n",
        "\tcontainer = np.load('/content/drive/My Drive/Chess/dataXtrain.npz')\n",
        "\tb = container['arr_0']\n",
        "\treturn b\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7cH7Lenf8Ms"
      },
      "source": [
        "x_train = get_dataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYXxtFCDhjX8"
      },
      "source": [
        "x_train[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f9RdNBJhbyp"
      },
      "source": [
        "x_train = np.asarray(x_train,dtype=np.int16)\n",
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqpqAIyigRBb"
      },
      "source": [
        "# i= 0\n",
        "# b = 0\n",
        "# x_train = []\n",
        "# count = 4126383-3\n",
        "\n",
        "# with open(\"/content/data.pgn\") as pgn:\n",
        "#     while 1:    \n",
        "#       try:\n",
        "#         game = chess.pgn.read_game(pgn)\n",
        "#       except Exception:\n",
        "#         break\n",
        "#       board = game.board()\n",
        "#       for move in game.mainline_moves():\n",
        "#         board.push(move)\n",
        "#         x_train.append(split_dims(board))\n",
        "#         if b > count:\n",
        "#           break\n",
        "#         b = b + 1\n",
        "        \n",
        "#       if b > count:\n",
        "#           break\n",
        "\n",
        "# x_train = np.asarray(x_train,dtype=np.int16)\n",
        "# x_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdFPA_BQJyep"
      },
      "source": [
        "a = 0\n",
        "count = 4126383-3\n",
        "y_train = []\n",
        "with open('/content/stockfish — копия.csv', newline='') as File:  \n",
        "    reader = csv.reader(File)\n",
        "    for row in reader:\n",
        "        row1 = str(row[1]).split()\n",
        "        if row1 == ['MoveScores']:\n",
        "          continue \n",
        "        #print(row1)\n",
        "        for i in str(row[1]).split():\n",
        "           y_train.append(i)\n",
        "           if a > count:\n",
        "             break\n",
        "           a = a + 1\n",
        "        if a > count:\n",
        "             break\n",
        "\n",
        "\n",
        "y_train = np.asarray(y_train,dtype=np.float32)\n",
        "y_train = np.asarray(y_train / abs(y_train).max() / 2 + 0.5, dtype=np.float32)\n",
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPhz6m_UJL71"
      },
      "source": [
        "4 126 382"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6S7QNZqwmBOP"
      },
      "source": [
        "def build_model(conv_depth):\n",
        "  board3d = layers.Input(shape=(14, 8, 8))\n",
        "\n",
        "  # adding the convolutional layers\n",
        "  x = board3d\n",
        "  x = layers.Conv2D(conv_depth, kernel_size=(1,1), padding='same', activation=tf.keras.layers.LeakyReLU(), data_format='channels_first')(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  #x = layers.Conv2D(conv_depth, kernel_size=(3,3), padding='same', activation='relu', data_format='channels_first')(x)#,data_format='channels_first')(x)\n",
        "  x = layers.MaxPool2D(pool_size=(2,2), data_format='channels_first')(x)\n",
        "  x = layers.Conv2D(conv_depth*2, kernel_size=(1,1), padding='same',activation=tf.keras.layers.LeakyReLU(),data_format='channels_first')(x)\n",
        " # x = layers.Conv2D(conv_depth*2, kernel_size=(3,3), padding='same', activation='relu',data_format='channels_first')(x)\n",
        "  #x = layers.MaxPool2D(pool_size=(2,2), data_format='channels_first')(x)\n",
        "  #x = layers.Conv2D(conv_depth*4, kernel_size=(2,2), padding='same', activation='relu', data_format='channels_first')(x)\n",
        "  #x = layers.Conv2D(conv_depth*3, kernel_size=(3,3), padding='same', activation=tf.keras.layers.LeakyReLU(), data_format='channels_first')(x)\n",
        "  x = layers.Flatten()(x)\n",
        "  # x = layers.Dense(64, activation=tf.keras.layers.LeakyReLU())(x)\n",
        "  # x = layers.Dropout(0.3)(x)\n",
        "  # x = layers.Dense(512, activation=tf.keras.layers.LeakyReLU())(x)\n",
        "  # x = layers.Dropout(0.3)(x)\n",
        "  # x = layers.Dense(32,activation=tf.keras.layers.LeakyReLU())(x)\n",
        "  # x = layers.Dropout(0.3)(x)\n",
        "  x = layers.Dense(1, 'sigmoid')(x)\n",
        "  return models.Model(inputs=board3d, outputs=x)\n",
        "\n",
        "  \n",
        "model = build_model(14)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyOYq9mv2ppC"
      },
      "source": [
        "i = 1\n",
        "All_history = []\n",
        "while i < 7:\n",
        "  model = build_model(14)\n",
        "  model.compile(optimizer=optimizers.Adam(), loss='mean_squared_error')\n",
        "  model.summary()\n",
        "  history = model.fit(x_train, y_train,   # TODO x_train, y_train change to ai traing \n",
        "            batch_size=512,\n",
        "            epochs=40,\n",
        "            verbose=1,\n",
        "            validation_split=0.1,\n",
        "            callbacks=[callbacks.ReduceLROnPlateau(monitor='loss', patience=10),\n",
        "                      callbacks.EarlyStopping(monitor='loss', patience=15)])\n",
        "\n",
        "  All_history.append(history)\n",
        "\n",
        "  i = i + 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7S6IQa5hofC"
      },
      "source": [
        "board = chess.Board(\"2r4k/p4b2/4pq2/1p1p1nR1/5P2/P2B4/1P2Q2P/1K4R1 b - - 2 30\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZ1tHx9fFQf-"
      },
      "source": [
        "board"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiP90oYbSk0u"
      },
      "source": [
        "def build_model_residual(conv_size, conv_depth):\n",
        "  board3d = layers.Input(shape=(14, 8, 8))\n",
        "\n",
        "  # adding the convolutional layers\n",
        "  x = layers.Conv2D(filters=conv_size, kernel_size=3, padding='same', data_format='channels_first')(board3d)\n",
        "  for _ in range(conv_depth):\n",
        "    previous = x\n",
        "    x = layers.Conv2D(filters=conv_size, kernel_size=3, padding='same', data_format='channels_first')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(tf.keras.layers.LeakyReLU())(x)\n",
        "    x = layers.Conv2D(filters=conv_size, kernel_size=3, padding='same', data_format='channels_first')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Add()([x, previous])\n",
        "    x = layers.Activation(tf.keras.layers.LeakyReLU())(x)\n",
        "  x = layers.Flatten()(x)\n",
        "  x = layers.Dense(64, activation=tf.keras.layers.LeakyReLU())(x)\n",
        "  x = layers.Dropout(0.3)(x)\n",
        "  x = layers.Dense(512, 'relu')(x)\n",
        "  x = layers.Dropout(0.3)(x)\n",
        "  x = layers.Dense(32, 'relu')(x)\n",
        "  x = layers.Dropout(0.3)(x)\n",
        "  x = layers.Dense(1, 'sigmoid')(x)\n",
        "\n",
        "  return models.Model(inputs=board3d, outputs=x)\n",
        "model_residual = build_model_residual(14,4)\n",
        "model_residual.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4siNk2yuBQzu"
      },
      "source": [
        "def step(X, y):\n",
        "\t# keep track of our gradients\n",
        "\twith tf.GradientTape() as tape:\n",
        "\t\t# make a prediction using the model and then calculate the\n",
        "\t\t# loss\n",
        "\t\tpred = model(X)\n",
        "\t\tloss = categorical_crossentropy(y, pred)\n",
        "\t# calculate the gradients using our tape and then update the\n",
        "\t# model weights\n",
        "\tgrads = tape.gradient(loss, model.trainable_variables)\n",
        "\topt.apply_gradients(zip(grads, model.trainable_variables))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5yqIP3sIrZ4"
      },
      "source": [
        "samples = 4000000\n",
        "# Instantiate an optimizer.\n",
        "optimizer = optimizers.Adam()\n",
        "# Instantiate a loss function.\n",
        "\n",
        "loss_fn = tf.compat.v1.losses.mean_squared_error\n",
        "\n",
        "# Prepare the training dataset.\n",
        "batch_size = 64\n",
        "\n",
        "\n",
        "# Reserve 10,000 samples for validation.\n",
        "x_val = x_train[samples:]\n",
        "y_val = y_train[samples:]\n",
        "x_train_final = x_train[:samples]\n",
        "y_train_final = y_train[:samples]\n",
        "\n",
        "# Prepare the training dataset.\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train_final, y_train_final))\n",
        "#train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "# Prepare the validation dataset.\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "val_dataset = val_dataset.batch(batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfwUQigkIQmq"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5q3XLvggMOnz"
      },
      "source": [
        "EPOCHS = 2\n",
        "#model = build_model_residual(14,4)\n",
        "model = build_model(14)\n",
        "BS = 512\n",
        "numUpdates = int(y_train.shape[0] / BS)\n",
        "history_grad = []\n",
        "sum_history = []\n",
        "  # Iterate over the batches of the dataset.\n",
        "for epoch in range(0, EPOCHS):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "\n",
        "        # Open a GradientTape to record the operations run\n",
        "        # during the forward pass, which enables auto-differentiation.\n",
        "        #with tf.GradientTape() as tape:\n",
        "\n",
        "            \n",
        "            # Run the forward pass of the layer.\n",
        "            # The operations that the layer applies\n",
        "            # to its inputs are going to be recorded\n",
        "            # on the GradientTape.\n",
        "    for i in range(0, numUpdates):\n",
        "              with tf.GradientTape() as tape:\n",
        "                start = i * BS\n",
        "                end = start + BS\n",
        "                logits = model(x_train_final[start:end], training=True)  # Logits for this minibatch\n",
        "                #logits = np.reshape(np.array(logits), (BS,))\n",
        "              # Compute the loss value for this minibatch.\n",
        "                loss_value = tf.keras.losses.MSE(y_train_final[start:end], logits)\n",
        "\n",
        "                # Use the gradient tape to automatically retrieve\n",
        "                # the gradients of the trainable variables with respect to the loss.\n",
        "                grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "\n",
        "                # Run one step of gradient descent by updating\n",
        "                # the value of the variables to minimize the loss.\n",
        "                optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "                #sum_history.append(np.average(grads))\n",
        "                for layers_grads in grads:\n",
        "                  sum_history.append(np.average(layers_grads))\n",
        "                history_grad.append(sum_history)\n",
        "                if start > 3980800:\n",
        "                  break\n",
        "\n",
        "                # Log every 200 batches.\n",
        "                #if i % 400 == 0:\n",
        "                    #print(\"Training loss (for one batch) at step %d: %.4f\"% (start, float(loss_value)))\n",
        "                    #print(\"Seen so far: %s samples\" % ((start + 1) * batch_size))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLGc-2oBGiM-"
      },
      "source": [
        "b=0\n",
        "for step, i in enumerate(grads):\n",
        "  print(i.shape)\n",
        "  b = step+1\n",
        "  #if  step > 100:\n",
        "    #break\n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szz4qHK9gkSl"
      },
      "source": [
        "layer_all = []\n",
        "num = b\n",
        "for i in range(0,b):\n",
        "  layer = []\n",
        "  for step, layers_grads in enumerate(history_grad[0]):\n",
        "    if ((step+1)%num == i):\n",
        "      layer.append(layers_grads)\n",
        "    #print(\"layer\",(step+1)%num,',', layers_grads)\n",
        "    if step > 1000:\n",
        "      break\n",
        "  layer_all.append(layer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMT-2QQ-QKJx"
      },
      "source": [
        "layer1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqjRuQKHfYZx"
      },
      "source": [
        "for i in model.trainable_weights:\n",
        "  print(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhNJyLDQQojl"
      },
      "source": [
        "plt.xlabel('Epochs')\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.plot(history.epoch, history.history[\"loss\"],  label='loss %s' %(b))\n",
        "plt.grid()\n",
        "plt.plot(history.epoch, history.history[\"val_loss\"],  label='val_loss %s' %(b))\n",
        "plt.legend()\n",
        "plt.xlim([0, max(history.epoch)+20])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzId2dNRDO-A"
      },
      "source": [
        "i = 0 #len(All_history)-1\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.plot(All_history[i].epoch, All_history[i].history[\"loss\"],  label='loss')\n",
        "plt.grid()\n",
        "plt.plot(All_history[i].epoch, All_history[i].history[\"val_loss\"],  label='val_loss' )\n",
        "plt.legend()\n",
        "plt.xlim([0, max(All_history[i].epoch)])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FJ4hT9DXjLr"
      },
      "source": [
        "for i in layer_all:\n",
        "  plt.plot(i)\n",
        "  plt.xlabel(\"layer\")\n",
        "  plt.xlabel(\"butch\")\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTVqyzZBXVn8"
      },
      "source": [
        "for i in layer_all:\n",
        "  plt.plot(i)\n",
        "  plt.xlabel(\"layer\")\n",
        "  plt.xlabel(\"butch\")\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gh-3m3MhGRXQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhqeVy2__6xs"
      },
      "source": [
        "for i in layer_all:\n",
        "  plt.plot(i)\n",
        "  plt.xlabel(\"layer\")\n",
        "  plt.xlabel(\"butch\")\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UndQQeUurAKp"
      },
      "source": [
        "# Playing with the AI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4CfjcGorHzg"
      },
      "source": [
        "# used for the minimax algorithm\n",
        "def minimax_eval(board):\n",
        "  board3d = split_dims(board)\n",
        "  board3d = np.expand_dims(board3d, 0)\n",
        "  return model.predict(board3d)[0][0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-JFZ2lUuLLY"
      },
      "source": [
        "with chess.engine.SimpleEngine.popen_uci('stockfish-5-linux/Linux/stockfish_14053109_x64') as engine:\n",
        "  while True:\n",
        "    h = []\n",
        "    move_answer = []\n",
        "    depth = 19\n",
        "    start = stockfish(board, depth)\n",
        "    d = depth\n",
        "    while(start is None):\n",
        "      d = d - 1\n",
        "      start = stockfish(board, d)\n",
        "      \n",
        "    for move in board.legal_moves:\n",
        "      board.push(move)\n",
        "      start_next = stockfish(board, depth)\n",
        "      d = depth\n",
        "      while (start_next is None):\n",
        "        d = d - 1\n",
        "        start_next = stockfish(board, d)      \n",
        "      h.append(start - start_next)#minimax_eval(board))\n",
        "      move_answer.append(move)\n",
        "      board.pop()     \n",
        "    board.push(move_answer[h.index(min(h))])\n",
        "    print(f'\\n{board}')\n",
        "    if board.is_game_over():\n",
        "      break\n",
        "\n",
        "    move = engine.analyse(board, chess.engine.Limit(time=0.1), info=chess.engine.INFO_PV)['pv'][0]\n",
        "    board.push(move)\n",
        "    print(f'\\n{board}')\n",
        "    if board.is_game_over():\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBKpEOs_etue"
      },
      "source": [
        "moves"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QNKBRvpkB6v"
      },
      "source": [
        "board = chess.Board()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jknvqyp6o5Nc"
      },
      "source": [
        "board.variation_san(moves)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pFCWt-1bGxa"
      },
      "source": [
        "moves = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxZgP5ROkOBp"
      },
      "source": [
        "moves.append(board.pop())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pImJTtdG-Iny"
      },
      "source": [
        "board\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxFjBumbXIid"
      },
      "source": [
        "board.push_san(\"a5\")\n",
        "\n",
        "board"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m70pFhj7qtO2"
      },
      "source": [
        "start_fish = (stockfish(board)+50)/100\n",
        "h = []\n",
        "move_answer = []\n",
        "for move in board.legal_moves:\n",
        "    board.push(move)\n",
        "    h.append(start_fish - (stockfish(board)+50)/100)\n",
        "    move_answer.append(move)\n",
        "    #print(move,abs(start_fish - (stockfish(board)+50)/100))\n",
        "    board.pop()\n",
        "print(move_answer[h.index(min(h))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmDzZ26CkaQo"
      },
      "source": [
        "def get_move(board):\n",
        "  h = []\n",
        "  move_answer = []\n",
        "  start = minimax_eval(board)\n",
        "  for move in board.legal_moves:\n",
        "      board.push(move)\n",
        "      h.append(abs(start - minimax_eval(board)))\n",
        "      move_answer.append(move)\n",
        "      #print(move,abs(start - minimax_eval(board)))\n",
        "      board.pop()\n",
        "  return move_answer[h.index(max(h))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aKTXEv6QZ9e"
      },
      "source": [
        "for i in range(30):\n",
        "  start_time = time.time()\n",
        "  print(i+1, \"depth ---  \",stockfish(board),\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdJkHx4HYxKE"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXb7Tyv4NqMI"
      },
      "source": [
        "\n",
        "h = []\n",
        "move_answer = []\n",
        "start = minimax_eval(board)\n",
        "for move in board.legal_moves:\n",
        "    start_time = time.time()\n",
        "    board.push(move)\n",
        "    h.append(start - minimax_eval(board))\n",
        "    move_answer.append(move)\n",
        "    print(move,abs(start - minimax_eval(board)) ,\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "    board.pop()\n",
        "print(move_answer[h.index(min(h))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_sTORJfGS9w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDnA7VTmGT7b"
      },
      "source": [
        "import pandas\n",
        "data = pandas.read_csv('/content/drive/My Drive/Chess/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqJhX5_LHFNl"
      },
      "source": [
        "/content/drive/My Drive/Chess/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4N5Bo1hAHF-H"
      },
      "source": [
        " pandas.DataFrame(data=data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OaKP0B9HjQx"
      },
      "source": [
        "line = \"rnbqkbnr/pppppppp/8/8/4P3/8/PPPP1PPP/RNBQKBNR b KQkq - 0 1,-10\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZyI1qYMINBR"
      },
      "source": [
        "a, b = line.split(',')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xeZci2EOMBK"
      },
      "source": [
        "chess.Board(fen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJxex2pwMpDE"
      },
      "source": [
        "!unzip /content/chessData.csv.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyyB5beYIVO6"
      },
      "source": [
        "a = 0\n",
        "count = 4126383-3\n",
        "y_train = []\n",
        "x_train = []\n",
        "with open('/content/chessData.csv', newline='') as File:  \n",
        "    reader = csv.reader(File)\n",
        "    for row in reader:\n",
        "        a = a + 1\n",
        "        if row == ['FEN', 'Evaluation']:\n",
        "          continue \n",
        "        fen, stfish = row[0], row[1]\n",
        "\n",
        "        fen = split_dims(chess.Board(fen))\n",
        "        x_train.append(fen)\n",
        "\n",
        "        if stfish[0]=='#':\n",
        "          if stfish[1] =='-':\n",
        "            stfish = \"-1000\"\n",
        "\n",
        "\n",
        "        print(stfish[0]=='#')\n",
        "        #print(row)      \n",
        "        if a > 100:\n",
        "          break\n",
        "\n",
        "\n",
        "# y_train = np.asarray(y_train,dtype=np.float32)\n",
        "# y_train = np.asarray(y_train / abs(y_train).max() / 2 + 0.5, dtype=np.float32)\n",
        "# y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-4IocQWPNqe"
      },
      "source": [
        "print(row[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWGfNxxxIWo2"
      },
      "source": [
        "b"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}